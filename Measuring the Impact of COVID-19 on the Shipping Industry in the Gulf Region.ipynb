{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 67-364 Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First and foremost, we have to download the python libraries and write the helper functions needed to produce the data; and they are as follows:\n",
    "#### 1) Tweepy: \n",
    "Twitter API used to gather Tweets from Twitter\n",
    "#### 2) Pandas, matplotlib, and seaborn: \n",
    "Data visualization libraries\n",
    "#### 3) Textblob, networkx, and nltk: \n",
    "Data analysis libraries\n",
    "#### 4) Others: \n",
    "libraries used to assist with the analysis step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: textblob in c:\\users\\andrew\\anaconda3\\lib\\site-packages (0.15.3)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\andrew\\anaconda3\\lib\\site-packages (from textblob) (3.4.5)\n",
      "Requirement already satisfied: six in c:\\users\\andrew\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.14.0)\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Andrew\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob\n",
    "from textblob import TextBlob\n",
    "\n",
    "import tweepy as tw\n",
    "from tweepy import OAuthHandler\n",
    "\n",
    "# from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import string\n",
    "\n",
    "import csv\n",
    "import urllib\n",
    "from pprint import pprint\n",
    "\n",
    "import itertools\n",
    "import collections\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import bigrams\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "#helper function used to remove the URL from a given string. This is helpful as the Tweets we get from the Twitter API often\n",
    "#contains the url of the Tweet, which could hinder the accuracy of our sentiment analysis tools.\n",
    "#Requires: a string\n",
    "#Ensures: The same txt string with url's removed.\n",
    "def remove_url(txt):\n",
    "    return \" \".join(re.sub(\"([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \"\", txt).split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are getting information from Twitter, authentication matters because it enables Twitter to keep their networks secure by permitting only authenticated users (or processes) to access its protected resources. For privacy reasons, we have commented out our key and access tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Hi\n"
     ]
    }
   ],
   "source": [
    "consumer_key = 'TwBvCfS4R1EieqZNPzhrnANHr'\n",
    "consumer_secret = 'sRdRTvOBrY9juvI0Sb03GS8rEvEN4M9l1vYqaxj1ZR6JOR3H8m'\n",
    "access_token = '3162328915-37LW2nu1fxi5dQIi9O41CA4M7C6E3qF0vQOLmrs'\n",
    "access_secret = 'Ce88SfJbCYMNqiD9ZSaEo6h9xiygtyNUGMJqKRZKizStj'\n",
    "\n",
    "try:\n",
    "    auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_secret)\n",
    "    api = tw.API(auth, wait_on_rate_limit=True)\n",
    "        \n",
    "except:\n",
    "    print(\"Error: Authentication Failed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Search words and Date\n",
    "# search_words = \"\"\"AramexUAE OR AramexQatar OR AramexKwi OR AramexQa OR Aramex_KSA OR UPS_Kuwait\"\"\"\n",
    "# date_since = \"2019-12-1\"\n",
    "\n",
    "# # To Keep or Remove Retweets\n",
    "# search_words = search_words + \" -filter:retweets\"\n",
    "\n",
    "# # Collect tweets\n",
    "# tweets = tw.Cursor(api.search,\n",
    "#                    q=search_words,\n",
    "#                    lang=\"en\",\n",
    "#                    tweet_mode = 'extended',\n",
    "#                    since=date_since).items(1000)\n",
    "\n",
    "# #Saving the tweets in a csv file:\n",
    "# csvFile = open(\"aramex_gcc_en.csv\", \"w+\")\n",
    "# csvWriter = csv.writer(csvFile)\n",
    "\n",
    "# #Writing the header of the csv file\n",
    "# csvWriter.writerow([\"Date\", \"Tweet\", \"User\", \"Location\"])\n",
    "\n",
    "# #general information saved for analysis\n",
    "# aramex_gcc_en = []\n",
    "\n",
    "# counter = 0\n",
    "\n",
    "# for tweet in tweets:\n",
    "#     time_created = tweet.created_at.strftime(\"%m/%d/%Y\")\n",
    "\n",
    "#     filtered_text = tweet.full_text\n",
    "    \n",
    "#     filtered_text = remove_url(filtered_text)\n",
    "    \n",
    "#     screen_user = (tweet.user.screen_name.encode('utf-8'))\n",
    "\n",
    "#     location = (tweet.user.location.encode('utf-8'))\n",
    "    \n",
    "#     if (\"thanks for reaching out\" not in filtered_text):\n",
    "#         aramex_gcc_en.append(tweet.full_text)\n",
    "#         counter += 1\n",
    "#         csvWriter.writerow([time_created, filtered_text, screen_user, location])\n",
    "        \n",
    "# csvFile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}