{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "import collections\n",
    "import string\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import gzip\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_19_tweets = pd.read_csv('COVID-19_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            Date                                              Tweet  \\\n",
       "0     11/18/2020  BREAKING Athletic Director Patricia Nicol has ...   \n",
       "1     11/18/2020  I need the audience to realize COVID19 was cir...   \n",
       "2     11/18/2020  thus it breaksdown and splits a chastisement t...   \n",
       "3     11/18/2020  Cant believe we just canceled the entire video...   \n",
       "4     11/18/2020  12 This year will be a tough holiday season We...   \n",
       "...          ...                                                ...   \n",
       "7930  11/13/2020  Covid19 is about to collapse our hospital syst...   \n",
       "7931  11/13/2020  Covid19 was the worst thing to happen to me me...   \n",
       "7932  11/13/2020  1 new COVID19 case in New Providence 24 total ...   \n",
       "7933  11/13/2020  What about blaming essential workers for the s...   \n",
       "7934  11/13/2020  MORE than 1400 People DIED in the US Yesterday...   \n",
       "\n",
       "                    User                 Location Belief Keywords_used  \n",
       "0      b'ethanmmcdowell'            b'Boston, MA'      .      keywords  \n",
       "1     b'salvadorandolar'    b'Stolen Tongva Land'      .      keywords  \n",
       "2     b'EDHKNse8ps3kkfa'                      b''      .      keywords  \n",
       "3           b'RyanPinks'      b'Philadelphia, PA'      .      keywords  \n",
       "4        b'joinandrewdo'    b'Orange County, CA '      .      keywords  \n",
       "...                  ...                      ...    ...           ...  \n",
       "7930   b'BeamMeUpScotee'           b'Oregon, USA'      .      keywords  \n",
       "7931         b'Kam_Bam_'               b'arkadoo'      .      keywords  \n",
       "7932     b'MayorMorgan1'    b'New Providence, NJ'      .      keywords  \n",
       "7933      b'AG_HighFly5'  b'Southside.Iowa City.'      .      keywords  \n",
       "7934      b'lac4justice'                      b''      .      keywords  \n",
       "\n",
       "[7935 rows x 6 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>Tweet</th>\n      <th>User</th>\n      <th>Location</th>\n      <th>Belief</th>\n      <th>Keywords_used</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>11/18/2020</td>\n      <td>BREAKING Athletic Director Patricia Nicol has ...</td>\n      <td>b'ethanmmcdowell'</td>\n      <td>b'Boston, MA'</td>\n      <td>.</td>\n      <td>keywords</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>11/18/2020</td>\n      <td>I need the audience to realize COVID19 was cir...</td>\n      <td>b'salvadorandolar'</td>\n      <td>b'Stolen Tongva Land'</td>\n      <td>.</td>\n      <td>keywords</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>11/18/2020</td>\n      <td>thus it breaksdown and splits a chastisement t...</td>\n      <td>b'EDHKNse8ps3kkfa'</td>\n      <td>b''</td>\n      <td>.</td>\n      <td>keywords</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>11/18/2020</td>\n      <td>Cant believe we just canceled the entire video...</td>\n      <td>b'RyanPinks'</td>\n      <td>b'Philadelphia, PA'</td>\n      <td>.</td>\n      <td>keywords</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11/18/2020</td>\n      <td>12 This year will be a tough holiday season We...</td>\n      <td>b'joinandrewdo'</td>\n      <td>b'Orange County, CA '</td>\n      <td>.</td>\n      <td>keywords</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7930</th>\n      <td>11/13/2020</td>\n      <td>Covid19 is about to collapse our hospital syst...</td>\n      <td>b'BeamMeUpScotee'</td>\n      <td>b'Oregon, USA'</td>\n      <td>.</td>\n      <td>keywords</td>\n    </tr>\n    <tr>\n      <th>7931</th>\n      <td>11/13/2020</td>\n      <td>Covid19 was the worst thing to happen to me me...</td>\n      <td>b'Kam_Bam_'</td>\n      <td>b'arkadoo'</td>\n      <td>.</td>\n      <td>keywords</td>\n    </tr>\n    <tr>\n      <th>7932</th>\n      <td>11/13/2020</td>\n      <td>1 new COVID19 case in New Providence 24 total ...</td>\n      <td>b'MayorMorgan1'</td>\n      <td>b'New Providence, NJ'</td>\n      <td>.</td>\n      <td>keywords</td>\n    </tr>\n    <tr>\n      <th>7933</th>\n      <td>11/13/2020</td>\n      <td>What about blaming essential workers for the s...</td>\n      <td>b'AG_HighFly5'</td>\n      <td>b'Southside.Iowa City.'</td>\n      <td>.</td>\n      <td>keywords</td>\n    </tr>\n    <tr>\n      <th>7934</th>\n      <td>11/13/2020</td>\n      <td>MORE than 1400 People DIED in the US Yesterday...</td>\n      <td>b'lac4justice'</td>\n      <td>b''</td>\n      <td>.</td>\n      <td>keywords</td>\n    </tr>\n  </tbody>\n</table>\n<p>7935 rows Ã— 6 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "covid_19_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(tweet):\n",
    "    #Step 1 - drop links\n",
    "    regex = re.compile(r'https?://t.co/[a-zA-Z0-9]{10}')\n",
    "    remove_links = re.split(regex,tweet)\n",
    "    remove_links = ' '.join(remove_links)\n",
    "\n",
    "    #Step 2 - Remove any non-ascii characters\n",
    "    remove_non_ascii =  re.sub(r'[^\\x00-\\x7F]+',' ', remove_links).lower()\n",
    "\n",
    "    #Step 3 - check for apostrophes\n",
    "    remove_apostroph = remove_non_ascii.replace(\"'s\",\"\")\n",
    "    remove_apostroph = remove_apostroph.replace(\"'\",\"\")\n",
    "    remove_apostroph = remove_apostroph.split(' ')\n",
    "    \n",
    "    #Step 4 - Remove dashes\n",
    "    remove_dashes = ' '.join(remove_apostroph).split('-')\n",
    "    remove_dashes = ' '.join(remove_dashes)\n",
    "\n",
    "    #Step 5 - Keep letters and digits only\n",
    "    keep_letters =  re.sub(r'[^a-zA-Z0-9]',' ', remove_dashes)\n",
    "\n",
    "    keep_letters = keep_letters.split(' ')\n",
    "\n",
    "    keep_letters = ' '.join(keep_letters)\n",
    "\n",
    "    #Step 6 - Tokenize \n",
    "    tokenize = nltk.word_tokenize(keep_letters)\n",
    "\n",
    "    #Step 7 - Lemmatize    \n",
    "    lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()\n",
    "    lemmatize_text = map(lambda x: lemmatizer.lemmatize(x), tokenize)\n",
    "\n",
    "    #Step 8 - Remove stop words\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    remove_stop = list(filter(lambda x: x not in stopwords, lemmatize_text))\n",
    "\n",
    "    #Step 9 - Remove any empty strings from list\n",
    "    processed_list = list(filter(lambda a: a != \"\", remove_stop))\n",
    "\n",
    "    processed_string = \" \".join(processed_list)\n",
    "\n",
    "    return processed_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(clean_tweet(covid_19_tweets['Tweet'][100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_list = covid_19_tweets['Tweet'].tolist()\n",
    "# clean_tweet(covid_19_tweets.iloc['Tweet'])\n",
    "for i,tweet in enumerate(tweets_list):\n",
    "    tweets_list[i] = clean_tweet(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  (0, 35)\t0.03097492585122846\n  (0, 31)\t0.05871754825658438\n  (0, 10)\t0.029615952483205088\n  (0, 32)\t0.05582769213654632\n  (0, 1)\t0.07390962796097907\n  (0, 2)\t0.058680463793106145\n  (0, 33)\t0.03452190026253273\n  (0, 16)\t0.14232896228746486\n  (0, 29)\t0.16611665454107094\n  (0, 23)\t0.029384754446254206\n  (0, 26)\t0.0889152924993178\n  (0, 25)\t0.2463029055423408\n  (0, 14)\t0.10992410954657496\n  (0, 13)\t0.32948157194411093\n  (0, 22)\t0.2534192671305025\n  (0, 18)\t0.060206733704782925\n  (0, 30)\t0.27484490533485323\n  (0, 0)\t0.4643602919334879\n  (0, 17)\t0.0899221602659032\n  (0, 24)\t0.19263402344105626\n  (0, 19)\t0.32827975231595447\n  (0, 21)\t0.03522572770493068\n  (0, 11)\t0.2740840268481674\n  (0, 15)\t0.35554678318163685\n  (0, 28)\t0.16582383555061248\n  :\t:\n  (7934, 34)\t0.10981143606423806\n  (7934, 35)\t0.11946088596766924\n  (7934, 31)\t0.13587345526386954\n  (7934, 10)\t0.04568789529198837\n  (7934, 32)\t0.06459313829702942\n  (7934, 1)\t0.11401879933680888\n  (7934, 2)\t0.06789382057655691\n  (7934, 33)\t0.13314048951907712\n  (7934, 16)\t0.08233794012316518\n  (7934, 29)\t0.2349092198299485\n  (7934, 23)\t0.11332807758610719\n  (7934, 26)\t0.11430643040179868\n  (7934, 25)\t0.2533108097719175\n  (7934, 14)\t0.190774764414943\n  (7934, 13)\t0.14824954695321724\n  (7934, 22)\t0.0651574121986943\n  (7934, 18)\t0.04643981899512863\n  (7934, 30)\t0.2967981423024419\n  (7934, 0)\t0.5899424358962451\n  (7934, 17)\t0.09248065987382507\n  (7934, 24)\t0.148586190122374\n  (7934, 19)\t0.21101255816434084\n  (7934, 11)\t0.2114117776044327\n  (7934, 15)\t0.3375350177472055\n  (7934, 28)\t0.1492241787567597\n"
     ]
    }
   ],
   "source": [
    "def do_nothing(x):\n",
    "    return x\n",
    "\n",
    "def tweet_features(tweets_list):\n",
    "\n",
    "    tfidf = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer= do_nothing,\n",
    "    preprocessor=do_nothing\n",
    "    )\n",
    "    vector = tfidf.fit_transform(tweets_list)\n",
    "    return vector\n",
    "\n",
    "print(tweet_features(tweets_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "       n_clusters=2, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=0, tol=0.0001, verbose=0)"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "numeric = tweet_features(tweets_list)\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(numeric)\n",
    "kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.4856942 , 0.01675673, 0.04265231, 0.0153635 , 0.0076918 ,\n",
       "        0.00887472, 0.00701808, 0.00533019, 0.00726711, 0.00426801,\n",
       "        0.03755127, 0.25928795, 0.05067212, 0.16956478, 0.15388626,\n",
       "        0.30733207, 0.05957682, 0.10786396, 0.08623763, 0.30168422,\n",
       "        0.01803561, 0.05403234, 0.1511904 , 0.09778793, 0.24522514,\n",
       "        0.24533317, 0.09494011, 0.00873379, 0.1913488 , 0.17828868,\n",
       "        0.23590002, 0.09493749, 0.07830751, 0.05694514, 0.01513646,\n",
       "        0.06822081, 0.01550335],\n",
       "       [0.50641723, 0.03739493, 0.04038307, 0.02555261, 0.01371184,\n",
       "        0.01315978, 0.01324844, 0.01095803, 0.0095516 , 0.00966215,\n",
       "        0.03433083, 0.24791652, 0.05859919, 0.13635421, 0.16059335,\n",
       "        0.40736182, 0.06230269, 0.07836896, 0.09284837, 0.20808612,\n",
       "        0.02028359, 0.05785216, 0.16364148, 0.09489492, 0.18546298,\n",
       "        0.21754325, 0.10125948, 0.00891784, 0.19598215, 0.19278952,\n",
       "        0.21018852, 0.09453644, 0.07173045, 0.06182038, 0.01886508,\n",
       "        0.08064182, 0.01204563]])"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "centers = kmeans.cluster_centers_\n",
    "centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "scatter() missing 1 required positional argument: 'y'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-8b42a2e7262f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumeric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: scatter() missing 1 required positional argument: 'y'"
     ]
    }
   ],
   "source": [
    "plt.scatter(numeric[:, ], s=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}